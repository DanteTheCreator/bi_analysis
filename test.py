from autogen import GroupChat, GroupChatManager, UserProxyAgent, ConversableAgent
import tempfile
from autogen import ConversableAgent, register_function
from autogen.coding import LocalCommandLineCodeExecutor
from pandas import DataFrame
from sqlalchemy import create_engine, text
import pandas as pd
import re

def run_query(query: str) -> DataFrame:
    engine = create_engine(
        "postgresql://gpt_test_user:dedismtyvneliparoli123@10.0.55.239:5432/postgres"
    )
    # Connect to the database
    connection = engine.connect()

    try:
        # Placeholder: Assume last message from the chat history is the SQL query
        safe_sql_query = query  # Ensure this step reflects actual chat output handling
        # Using `text` to safely handle the SQL command
        sql_command = text(safe_sql_query)

        # Execute the query and fetch all results
        result_proxy = connection.execute(sql_command)
        records = result_proxy.fetchall()

        # Convert the results into a DataFrame
        if records:
            df = pd.DataFrame(records, columns=result_proxy.keys())  # Create DataFrame with data and headers
        else:
            df = None  # Create empty DataFrame with headers

        return df

    except Exception as e:
        print("An error occurred:", e)

    finally:
        # Close the connection
        connection.close()
        print("Success, database connection is closed")

def extract_sql(response_text):
    # Define a pattern that matches SQL queries enclosed in ```sql ``` format.
    pattern = r"```sql\s(.*?)```"

    # Search for the pattern using DOTALL flag to match across multiple lines
    match = re.search(pattern, response_text, re.DOTALL)

    if match:
        # Extract the SQL query without the enclosing triple backticks and 'sql'
        sql_query = match.group(1).strip()
        return sql_query
    else:
        return "No SQL query found in the response."

def extract_python_code(response_text):
    # Define a pattern that matches Python code enclosed in ```python ``` format.
    pattern = r"```python\s(.*?)```"

    # Search for the pattern using DOTALL flag to match across multiple lines
    match = re.search(pattern, response_text, re.DOTALL)

    if match:
        # Extract the Python code without the enclosing triple backticks and 'python'
        python_code = match.group(1).strip()
        return python_code
    else:
        return "No Python code found in the response."

# Create a temporary directory to store the code files.
temp_dir = tempfile.TemporaryDirectory()
# Create a local command line code executor.
executor = LocalCommandLineCodeExecutor(
    timeout=10,  # Timeout for each code execution in seconds.
    work_dir=temp_dir.name,  # Use the temporary directory to store the code files.
)

user_proxy = UserProxyAgent(
    name="user_proxy",
    llm_config=False,
    code_execution_config=False,
    human_input_mode="NEVER",
)

# Create Executor Agent
executor_proxy_agent = UserProxyAgent(
    "executor_agent",
    llm_config=False,  # Turn off LLM for this agent.
    is_termination_msg=lambda msg: msg.get("content") is not None and "TERMINATE" in msg["content"],
    code_execution_config={
        "executor": executor
    },  # Use the local command line code executor.
    human_input_mode="NEVER",  # Always take human input for this agent for safety.
)

mistral = {
    "config_list": [
        {
            "model": "gpt-3.5-turbo",
            "base_url": "https://api.openai.com/v1/",
            "api_key": "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",
        },
        
    ],
    "cache_seed": None,  # Disable caching.
    "temperature": 0.1
}

decomposer = ConversableAgent(
    name="Decomposer",
    system_message="""
    
    Given a user's description of the data they need, decompose the task and determine whether it can be accomplished using only SQL queries or if it requires both SQL queries and Python post-processing.
    If both SQL queries and Python post-processing are required: a. Provide detailed instructions for the Query Builder, as described in step 2a. b. Provide a description of the expected DataFrame that will be generated by the SQL query, including the column names and data types. c. Provide detailed instructions for the Script Builder, outlining the necessary data transformations, calculations, aggregations, visualizations, or any other post-processing steps required. Refer to the column names from the DataFrame description when specifying the post-processing tasks.
    Evaluate the decomposition to ensure it accurately captures the requirements and provides an efficient approach to retrieve and analyze the data.
    Deliver the instructions to the respective builders (Query Builder and/or Script Builder) for them to write the necessary SQL queries and/or Python code.
    1. Specific data retrieval queries needed to extract the right data from the database. 
    According to this schema:
    public.test_transactions_master_aggregated
                      (
 column_name                 data_type
0           transaction_id                    bigint
1              customer_id                   integer
2         transaction_date  timestamp with time zone
3                trans_val          double precision
4                  balance          double precision
5         reference_object                   integer
6  reference_object_bigint                    bigint
7                   status         character varying ( status can be one of these:
0         bbet
1          Bet
2        Bonus
3      Deposit
4          Fee
5  Withdrawals
6          Won
7         wwon));
    
    """,
    llm_config=mistral,
)

# Define Builder Agent
query_builder = ConversableAgent(
    name="query_builder",
    description="builds SQL query",
    human_input_mode='NEVER',
    system_message="""
    You will recieve a task and focus on **Database Operations**. There will be a detailed plan for building a PostgreSQL query.
    Given a detailed description of the required data fields, tables, and conditions extracted by the Decomposer, construct SQL queries that accurately retrieve this data from the database. Include:

        1. Selection of relevant fields.
        2. Identification of necessary tables and how they join, if applicable.
        3. Specification of conditions and filters to apply to the data.
        4. Any sorting or grouping operations that are required.

    Generate clean and efficient SQL code that is ready to be executed to fetch the needed data from this table:
    public.test_transactions_master_aggregated
                    (
 column_name                 data_type
0           transaction_id                    bigint
1              customer_id                   integer
2         transaction_date  timestamp with time zone
3               trans_val          double precision
4                  balance          double precision
5         reference_object                   integer
6  reference_object_bigint                    bigint
7                   status         character varying ( status is same as transactin type and can take one of these values:
0         bbet
1          Bet
2        Bonus
3      Deposit
4          Fee
5  Withdrawals
6          Won
7         wwon)
                    );
        
    IGNORE Python post-processing part.
    Provide single clean sql query in this format ```sql ```. Here are the examples:
    
    1) ```sql
    SELECT * FROM public.test_transactions_master_aggregated;```
    Explanation: This query retrieves all columns and all rows from the test_transactions_master_aggregated table. It's the simplest form of a SELECT statement and is used to display the entire content of a table.
    2) ```sql
    SELECT transaction_id, customer_id, trans_val, balance
    FROM public.test_transactions_master_aggregated
    WHERE balance > 1000.0;```
    Explanation: This query selects specific columns (transaction_id, customer_id, trans_val, balance) from the table, but only where the balance is greater than 1000.0. This is useful for filtering records based on specific criteria.
    3) ```sql
    SELECT customer_id, COUNT(*) AS number_of_transactions, SUM(trans_val) AS total_spent
    FROM public.test_transactions_master_aggregated
    GROUP BY customer_id
    HAVING SUM(trans_val) > 5000;```
    Explanation: This query groups the data by customer_id and calculates two things: the total number of transactions and the total transaction value (trans_val) per customer. The HAVING clause further filters these groups to include only those customers whose total spent is greater than 5000. This is useful for summarizing data by a certain attribute.
    4) ```sql
    SELECT transaction_id, transaction_date, trans_val
    FROM public.test_transactions_master_aggregated
    WHERE transaction_date BETWEEN '2024-01-01' AND '2024-12-31'
    ORDER BY transaction_date DESC;```
    Explanation: This query fetches the transaction ID, date, and value of transactions that occurred within the year 2024. The results are sorted by the transaction_date in descending order, so the most recent transactions appear first. This type of query is useful for analyzing data within a specific time frame.
    5) ```sql
    SELECT transaction_id, trans_val, customer_id
    FROM public.test_transactions_master_aggregated
    WHERE customer_id IN (
        SELECT customer_id
        FROM public.test_transactions_master_aggregated
        WHERE trans_val > 1000
        GROUP BY customer_id
        HAVING COUNT(transaction_id) > 5
    )
    ORDER BY trans_val DESC;```
    Explanation: This query selects transactions from customers who have more than five transactions exceeding a value of 1000. It uses a subquery in the WHERE clause to first identify those customer_ids meeting the criteria, and then fetches data from the main table for those customers. This is a more complex SQL operation that involves nested querying, useful for filtering data based on aggregate properties.
    7) ```sql
    SELECT transaction_id,
        transaction_date,
        customer_id,
        trans_val,
        SUM(trans_val) OVER (PARTITION BY customer_id ORDER BY transaction_date) AS running_total
    FROM public.test_transactions_master_aggregated
    ORDER BY customer_id, transaction_date;```
    Explanation: This query calculates a running total of transaction values for each customer, ordered by the transaction date. It uses a window function (SUM() OVER) which is a powerful tool for performing calculations across sets of rows that are related to the current row. Query 10: Complex Join with Aggregate and Filter
    """,
    llm_config=mistral,
)

# Define Validator Agent
script_builder = ConversableAgent(
    name="script_builder",
    description="builds python script",
    system_message=""" 
    You are an expert data scientist who codes in Python . Upon user request, write python script to transform df or create plots. for example, in user input your instruction comes after 'Python Script:'. and to get the sense about the data, you can use info after 'DataFrame Description:'; Dataset Name is df, assume it will be passed into the code you write, USE NAME DF, avoid changing this variable.
    Based on the post-processing needs identified by the Decomposer, create a Python script that performs the following operations on the data retrieved from the database:
        
    Note: Data will already be a pandas DataFrame, avoid creating sample data in the script. 

    Make sure that Python script NEVER includes 'while true' and other operations which could possibly be recursive or never stop.
    If you write 'while True' in a code, I will die.
    Ensure the script is modular, well-commented, and easy to integrate with the data retrieval outputs.
    Examples:
    Shot 1:
    Decomposition -> DataFrame Description:
        Columns:
        customer_id: integer
        total_transaction_value: double precision
        avg_transaction_value: double precision
        Python Script:
        Sort the DataFrame by the 'total_transaction_value' column in descending order.
        Select the top 10 customers.
        Create a bar chart using matplotlib with 'customer_id' on the x-axis and 'total_transaction_value' on the y-axis.
        Set appropriate labels and title for the chart.
    Expected python code:
        import matplotlib.pyplot as plt df = df.sort_values('total_transaction_value', ascending=False) top_10_customers = df.head(10)  plt.figure(figsize=(12, 6)) plt.bar(top_10_customers['customer_id'], top_10_customers['total_transaction_value']) plt.xlabel('Customer ID') plt.ylabel('Total Transaction Value') plt.title('Top 10 Customers by Total Transaction Value (Last 30 Days)') plt.xticks(rotation=45) plt.show()
    Shot 2:
    Decompostion -> 
        DataFrame Description:
        Columns:
        customer_id: integer
        distinct_transaction_types: bigint
        total_transaction_value: double precision
        Python Script:
        Create a histogram using matplotlib to visualize the distribution of 'distinct_transaction_types'.
        Set appropriate labels and title for the histogram.
    Expected python Output:
    import matplotlib.pyplot as plt plt.figure(figsize=(10, 6)) plt.hist(df['distinct_transaction_types'], bins=range(df['distinct_transaction_types'].min(), df['distinct_transaction_types'].max() + 2)) plt.xlabel('Number of Transaction Types') plt.ylabel('Number of Customers') plt.title('Distribution of Transaction Types per Customer') plt.xticks(range(df['distinct_transaction_types'].min(), df['distinct_transaction_types'].max() + 1)) plt.show()
    Shot 3:
    Decomposition -> 
    DataFrame Description:
        Columns:
        transaction_month: timestamp
        total_transaction_value: double precision
        Python Script:
        Calculate the percentage change in total transaction value from the previous month and add a new column 'pct_change' to the DataFrame.
        Create a line chart using matplotlib with 'transaction_month' on the x-axis and 'total_transaction_value' on the y-axis.
        Set appropriate labels and title for the chart.
        Display the results in a formatted table, including the month, total transaction value, and percentage change.
    Expected Python Output:
    import matplotlib.pyplot as plt df['pct_change'] = df['total_transaction_value'].pct_change() * 100 df['pct_change'] = df['pct_change'].fillna(0).round(2)  plt.figure(figsize=(12, 6)) plt.plot(df['transaction_month'], df['total_transaction_value'], marker='o') plt.xlabel('Month') plt.ylabel('Total Transaction Value') plt.title('Monthly Transaction Value Trend for Customer 12345') plt.xticks(rotation=45) plt.grid(True)  print("Month\t\tTotal Transaction Value\tPercentage Change") for index, row in df.iterrows():     print(f"{row['transaction_month'].strftime('%Y-%m')}\t\t{row['total_transaction_value']:.2f}\t\t\t{row['pct_change']:.2f}%")
""",
    llm_config=mistral,
    human_input_mode="NEVER",
    code_execution_config=False
)



decomposition = user_proxy.initiate_chat(
    recipient=decomposer,
    message="Write SQL query to get everything from transactions limit 10 and use that as a dataframe in python. Using 'while true' in python print that dataframe eternally",
    max_turns=1,

    # summary_method="reflection_with_llm",
)

query = user_proxy.initiate_chat(
    recipient=query_builder,
    message=decomposition.summary,
    max_turns=1,
)

query = extract_sql(query.summary)
df = run_query(query)

script_to_run = ''

if df is not None:
    final_result = user_proxy.initiate_chat(
        script_builder,
        max_turns=1,
        message=decomposition.summary,
    )
    
    script_to_run = extract_python_code(final_result.summary)
    print(script_to_run)
else:
    print('No data found')

exec(script_to_run, globals(), locals())