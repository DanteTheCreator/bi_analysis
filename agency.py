from autogen import UserProxyAgent, ConversableAgent


class Agency:
    def __init__(self,):
        self.llm_config = {
            "config_list": [
                {
                    "model": "gpt-3.5-turbo",
                    "base_url": "https://api.openai.com/v1/",
                    "api_key": "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",
                },

            ],
            "cache_seed": None,  # Disable caching.
            "temperature": 0.1
        }

        self.user_proxy = UserProxyAgent(
            name="user_proxy",
            llm_config=False,
            code_execution_config=False,
            human_input_mode="NEVER",
        )

        self.decomposer = ConversableAgent(
            name="Decomposer",
            system_message="""
    
            Given a user's description of the data they need, decompose the task and determine whether it can be accomplished using only SQL queries or if it requires both SQL queries 
            and Python post-processing.
            If both SQL queries and Python post-processing are required: 
            a. Provide detailed instructions for the Query Builder.
            b. Provide a description of the expected DataFrame that will be generated by the SQL query, including the column names and data types.
            c. Provide detailed instructions for the Script Builder, outlining the necessary data transformations, calculations, aggregations, visualizations,
            or any other post-processing steps required. Refer to the column names from the DataFrame description when specifying the post-processing tasks.
            
            Evaluate the decomposition to ensure it accurately captures the requirements and provides an efficient approach to retrieve and analyze the data.
            Deliver the instructions to the respective builders (Query Builder and/or Script Builder) for them to write the necessary SQL queries and Python code (if necessary).
            Avoid provideing Specific data retrieval queries needed to extract the right data from the database, Provide the instructions for generating such queries. 
            According to this schema:
            public.test_transactions_master_aggregated
                            (
                                column_name                 data_type
                                0           transaction_id                    bigint
                                1              customer_id                   integer
                                2         transaction_date  timestamp with time zone
                                3                trans_val          double precision
                                4                  balance          double precision
                                5         reference_object                   integer
                                6  reference_object_bigint                    bigint
                                7                   status         character varying ( status can be one of these:
                                0         bbet
                                1          Bet
                                2        Bonus
                                3      Deposit
                                4          Fee
                                5  Withdrawals
                                6          Won
                                7         wwon));
    
    
    ----------
    If further data transformation is needed, address script_builder and explain what post-processing steps it nees to complete.
    
    """,
            llm_config=self.llm_config,
        )

        self.query_builder = ConversableAgent(
            name="query_builder",
            description="builds SQL query",
            human_input_mode='NEVER',
            system_message="""
            You will recieve a task and focus on **Database Operations**. There will be a detailed plan for building a PostgreSQL query.
            Given a detailed description of the required data fields, tables, and conditions extracted by the Decomposer, construct SQL queries that accurately retrieve this data from the database. Include:

                1. Selection of relevant fields.
                2. Identification of necessary tables and how they join, if applicable.
                3. Specification of conditions and filters to apply to the data.
                4. Any sorting or grouping operations that are required.

            Generate clean and efficient SQL code that is ready to be executed to fetch the needed data from this table:
            public.test_transactions_master_aggregated
                            (
        column_name                 data_type
        0           transaction_id                    bigint
        1              customer_id                   integer
        2         transaction_date  timestamp with time zone
        3               trans_val          double precision
        4                  balance          double precision
        5         reference_object                   integer
        6  reference_object_bigint                    bigint
        7                   status         character varying ( status is same as transactin type and can take one of these values:
        0         bbet
        1          Bet
        2        Bonus
        3      Deposit
        4          Fee
        5  Withdrawals
        6          Won
        7         wwon)
                            );
                
            IGNORE Python post-processing part.
            Provide single clean sql query in this format ```sql ```. Here are the examples:
            
            1) ```sql
            SELECT * FROM public.test_transactions_master_aggregated;```
            Explanation: This query retrieves all columns and all rows from the test_transactions_master_aggregated table. It's the simplest form of a SELECT statement and is used to display the entire content of a table.
            2) ```sql
            SELECT transaction_id, customer_id, trans_val, balance
            FROM public.test_transactions_master_aggregated
            WHERE balance > 1000.0;```
            Explanation: This query selects specific columns (transaction_id, customer_id, trans_val, balance) from the table, but only where the balance is greater than 1000.0. This is useful for filtering records based on specific criteria.
            3) ```sql
            SELECT customer_id, COUNT(*) AS number_of_transactions, SUM(trans_val) AS total_spent
            FROM public.test_transactions_master_aggregated
            GROUP BY customer_id
            HAVING SUM(trans_val) > 5000;```
            Explanation: This query groups the data by customer_id and calculates two things: the total number of transactions and the total transaction value (trans_val) per customer. The HAVING clause further filters these groups to include only those customers whose total spent is greater than 5000. This is useful for summarizing data by a certain attribute.
            4) ```sql
            SELECT transaction_id, transaction_date, trans_val
            FROM public.test_transactions_master_aggregated
            WHERE transaction_date BETWEEN '2024-01-01' AND '2024-12-31'
            ORDER BY transaction_date DESC;```
            Explanation: This query fetches the transaction ID, date, and value of transactions that occurred within the year 2024. The results are sorted by the transaction_date in descending order, so the most recent transactions appear first. This type of query is useful for analyzing data within a specific time frame.
            5) ```sql
            SELECT transaction_id, trans_val, customer_id
            FROM public.test_transactions_master_aggregated
            WHERE customer_id IN (
                SELECT customer_id
                FROM public.test_transactions_master_aggregated
                WHERE trans_val > 1000
                GROUP BY customer_id
                HAVING COUNT(transaction_id) > 5
            )
            ORDER BY trans_val DESC;```
            Explanation: This query selects transactions from customers who have more than five transactions exceeding a value of 1000. It uses a subquery in the WHERE clause to first identify those customer_ids meeting the criteria, and then fetches data from the main table for those customers. This is a more complex SQL operation that involves nested querying, useful for filtering data based on aggregate properties.
            7) ```sql
            SELECT transaction_id,
                transaction_date,
                customer_id,
                trans_val,
                SUM(trans_val) OVER (PARTITION BY customer_id ORDER BY transaction_date) AS running_total
            FROM public.test_transactions_master_aggregated
            ORDER BY customer_id, transaction_date;```
            Explanation: This query calculates a running total of transaction values for each customer, ordered by the transaction date. It uses a window function (SUM() OVER) which is a powerful tool for performing calculations across sets of rows that are related to the current row. Query 10: Complex Join with Aggregate and Filter
            """,
            llm_config=self.llm_config,
        )

        self.script_builder = ConversableAgent(
            name="script_builder",
            description="builds python script",
            system_message=""" 
            You are an expert data scientist who codes in Python. Upon user request, write python script to transform df or create visualizations. 
            Assume that variable 'df' will be passed in automatically. Avoid creating or renaming variables, start with 'df', transform it however you need and make visualisations based on that data.
            
            Decomposer will describe the task well and tell you what to do. After 'Python Instructions:' or 'Post processing Instructions:' you will see your task.
                
            Note: Data will already be a pandas DataFrame, avoid creating sample data in the script. 
            
            Here are some examples:
            
            # Adding a new column based on existing data
            df['A_squared'] = df['A'] ** 2
            
            # Filtering rows where 'B' is greater than 0
            filtered_df = df[df['B'] > 0]
            
            # Grouping by 'Category' and calculating mean values
            grouped_df = df.groupby('Category').mean()
            
            # Visualization 1: Scatter plot of 'A_squared' vs 'B'
            plt.figure(figsize=(10, 6))
            sns.scatterplot(data=filtered_df, x='A_squared', y='B', hue='Category')
            plt.title('Scatter Plot of A_squared vs B')
            plt.show()

            # Visualization 2: Bar plot of average 'C' by 'Category'
            plt.figure(figsize=(10, 6))
            grouped_df['C'].plot(kind='bar', color='skyblue')
            plt.title('Average C by Category')
            plt.xlabel('Category')
            plt.ylabel('Average of C')
            plt.show()

            # Visualization 3: Box plot of 'D'
            plt.figure(figsize=(10, 6))
            sns.boxplot(x='Category', y='D', data=df)
            plt.title('Box Plot of D by Category')
            plt.show()

            Depending on decomposers task explanation, decide what would be the best transformation and visualization for the df.
            Provide pure python code.
        """,
            llm_config=self.llm_config,
            human_input_mode="NEVER",
            code_execution_config=False
        )
